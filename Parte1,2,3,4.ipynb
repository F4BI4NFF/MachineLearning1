{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1\n",
    "\n",
    "### Integrantes: Fernanda Weiss y Fabián Fernández"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el presente se trabajará con el dataset *House Sales in King County, USA* el cuál se utilizará en el presente trabajo para evaluar modelos simples de regresión líneal. El dataset nos entrega información sobre distintas características asociadas a las ventas de casas en la localidad King County, en el rango de un año (entre mayo de 2014 y mayo de 2015), como la cantidad de habitaciones, cantidad de baños, número de pisos, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Construcción de dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: <> (a. Construya un dataframe con los datos a analizar descargándolos desde la plataforma como se indic´o. Explique por qu´e se realiza la l´ınea 4.)\n",
    "\n",
    "Comenzaremos construyendo un dataframe con los datos descritos previamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      price  bedrooms  bathrooms  sqft_living  sqft_lot  floors  waterfront  \\\n",
       "0  221900.0         3       1.00         1180      5650     1.0           0   \n",
       "1  538000.0         3       2.25         2570      7242     2.0           0   \n",
       "2  180000.0         2       1.00          770     10000     1.0           0   \n",
       "3  604000.0         4       3.00         1960      5000     1.0           0   \n",
       "4  510000.0         3       2.00         1680      8080     1.0           0   \n",
       "\n",
       "   view  condition  grade  sqft_above  sqft_basement  yr_built  yr_renovated  \\\n",
       "0     0          3      7        1180              0      1955             0   \n",
       "1     0          3      7        2170            400      1951          1991   \n",
       "2     0          3      6         770              0      1933             0   \n",
       "3     0          5      7        1050            910      1965             0   \n",
       "4     0          3      8        1680              0      1987             0   \n",
       "\n",
       "       lat     long  sqft_living15  sqft_lot15  \n",
       "0  47.5112 -122.257           1340        5650  \n",
       "1  47.7210 -122.319           1690        7639  \n",
       "2  47.7379 -122.233           2720        8062  \n",
       "3  47.5208 -122.393           1360        5000  \n",
       "4  47.6168 -122.045           1800        7503  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"kc_house_data.csv\")\n",
    "df.drop(['id','date','zipcode',],axis=1,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante notar que al construir el dataframe se realiza una modificación a los datos originales en la línea 4, la que realiza lo siguiente:\n",
    "\n",
    "* Se eliminan las columnas que no agregan valor al análisis que se realizará, estas columnas son el *identificador (id), fecha (date) y cógido zip* (zipcode), esto es, no será posible realizar una estimación del precio con estas variables.\n",
    "\n",
    "\n",
    "\n",
    "* Mediante la opción *axis=1* se especifica el eje a través del cual se hará la acción, es decir en fila o columna. Por defecto se tiene *axis=0* que indica que se realice por fila, al indicar *axis=1* se indica que se realice por columna.\n",
    "[//]: <> (It specifies the axis along which the means are computed. By default axis=0. This is consistent with the numpy.mean usage when axis is specified explicitly -in numpy.mean, axis==None by default, which computes the mean  value over the flattened array , in which axis=0 along the rows -namely, index in pandas, and axis=1 along the columns. https://stackoverflow.com/questions/22149584/what-does-axis-in-pandas-mean Note: axis=1 denotes that we are referring to a column, not a row )\n",
    "* Mediante la opción *inplace* se indica que la modificación se realice directamente en el dataframe, sin tener que reasignarlo.\n",
    "[//]: <> (https://stackoverflow.com/questions/13411544/delete-column-from-pandas-dataframe-using-python-del)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Descripción de dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: <> (b. Describa brevemente el dataset a utilizar.)\n",
    "Luego se describe brevemente el dataset a utilizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21613 entries, 0 to 21612\n",
      "Data columns (total 18 columns):\n",
      "price            21613 non-null float64\n",
      "bedrooms         21613 non-null int64\n",
      "bathrooms        21613 non-null float64\n",
      "sqft_living      21613 non-null int64\n",
      "sqft_lot         21613 non-null int64\n",
      "floors           21613 non-null float64\n",
      "waterfront       21613 non-null int64\n",
      "view             21613 non-null int64\n",
      "condition        21613 non-null int64\n",
      "grade            21613 non-null int64\n",
      "sqft_above       21613 non-null int64\n",
      "sqft_basement    21613 non-null int64\n",
      "yr_built         21613 non-null int64\n",
      "yr_renovated     21613 non-null int64\n",
      "lat              21613 non-null float64\n",
      "long             21613 non-null float64\n",
      "sqft_living15    21613 non-null int64\n",
      "sqft_lot15       21613 non-null int64\n",
      "dtypes: float64(5), int64(13)\n",
      "memory usage: 3.0 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Return a tuple representing the dimensionality of the DataFrame\n",
    "print \"info\"\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con df.info() es posible ver la información asociada a cada columna, la cantidad de datos, si tiene algún dato nulo, y el tipo de dato. Los atributos para predecir el valor de una casa están especificados como:\n",
    "\n",
    "<ul>\n",
    "    <li><strong>price</strong> : Corresponda al precio de la casa, es la predicción objetivo del modelo. </li>\n",
    "    <li><strong>bedrooms</strong> : Número de dormitorios por casa. </li>\n",
    "    <li><strong>bathrooms</strong> : Número de baños por dormitorio.</li>\n",
    "    <li><strong>sqft_living</strong> : Pies cuadrados de la casa. </li>\n",
    "    <li><strong>sqft_lot</strong> : Pies de cuadrados del terreno. </li>\n",
    "    <li><strong>floors</strong> : Total de pisos en la casa. </li>\n",
    "    <li><strong>waterfront</strong> : Casa que tiene una vista al mar. </li>\n",
    "    <li><strong>view</strong> : Cantidad de veces que ha sido vista la casa. </li>\n",
    "    <li><strong>condition</strong> : Que tan buena es la condición de la casa. </li>\n",
    "    <li><strong>grade</strong> : grado general dado a la unidad de vivienda, basado en el sistema de clasificación del condado        de King.  </li>\n",
    "    <li><strong>sqft_above</strong>: Pies cuadrados aparte del sótano. </li>\n",
    "    <li><strong>sqtf_basement</strong> : Pies cuadrados del sótano. </li>\n",
    "    <li><strong>yr_built</strong> : Año de construcción de la vivienda. </li>\n",
    "    <li><strong>yr_renovate</strong> : Año en que la casa fue renovada. </li>\n",
    "    <li><strong>lat</strong> : Coordenada de latitud. </li>\n",
    "    <li><strong>long</strong> : Coordenada de longitud. </li>\n",
    "    <li><strong>sqft_living15</strong>: Área de la sala de estar en 2015 ( implica alguna renovación). Esto podría o no haber       afectado al área del terreno. </li>\n",
    "    <li><strong>sqft_lot15</strong> : Área del tamaño del terreno en 2015 ( implica alguna renovación).</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (21613, 18)\n"
     ]
    }
   ],
   "source": [
    "print \"shape:\",df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El comando df.shape visualiza la dimensión del dataframe. La cual consiste en 21613 datos entre el conjunto de entrenamiento y de prueba. Cada dato contiene 18 atributos, los anteriormente descritos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descripción\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.161300e+04</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>2.161300e+04</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.400881e+05</td>\n",
       "      <td>3.370842</td>\n",
       "      <td>2.114757</td>\n",
       "      <td>2079.899736</td>\n",
       "      <td>1.510697e+04</td>\n",
       "      <td>1.494309</td>\n",
       "      <td>0.007542</td>\n",
       "      <td>0.234303</td>\n",
       "      <td>3.409430</td>\n",
       "      <td>7.656873</td>\n",
       "      <td>1788.390691</td>\n",
       "      <td>291.509045</td>\n",
       "      <td>1971.005136</td>\n",
       "      <td>84.402258</td>\n",
       "      <td>47.560053</td>\n",
       "      <td>-122.213896</td>\n",
       "      <td>1986.552492</td>\n",
       "      <td>12768.455652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.671272e+05</td>\n",
       "      <td>0.930062</td>\n",
       "      <td>0.770163</td>\n",
       "      <td>918.440897</td>\n",
       "      <td>4.142051e+04</td>\n",
       "      <td>0.539989</td>\n",
       "      <td>0.086517</td>\n",
       "      <td>0.766318</td>\n",
       "      <td>0.650743</td>\n",
       "      <td>1.175459</td>\n",
       "      <td>828.090978</td>\n",
       "      <td>442.575043</td>\n",
       "      <td>29.373411</td>\n",
       "      <td>401.679240</td>\n",
       "      <td>0.138564</td>\n",
       "      <td>0.140828</td>\n",
       "      <td>685.391304</td>\n",
       "      <td>27304.179631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.500000e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>290.000000</td>\n",
       "      <td>5.200000e+02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>290.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1900.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.155900</td>\n",
       "      <td>-122.519000</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>651.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.219500e+05</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1427.000000</td>\n",
       "      <td>5.040000e+03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1190.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1951.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.471000</td>\n",
       "      <td>-122.328000</td>\n",
       "      <td>1490.000000</td>\n",
       "      <td>5100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.500000e+05</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1910.000000</td>\n",
       "      <td>7.618000e+03</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1560.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1975.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.571800</td>\n",
       "      <td>-122.230000</td>\n",
       "      <td>1840.000000</td>\n",
       "      <td>7620.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.450000e+05</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2550.000000</td>\n",
       "      <td>1.068800e+04</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2210.000000</td>\n",
       "      <td>560.000000</td>\n",
       "      <td>1997.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.678000</td>\n",
       "      <td>-122.125000</td>\n",
       "      <td>2360.000000</td>\n",
       "      <td>10083.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.700000e+06</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>13540.000000</td>\n",
       "      <td>1.651359e+06</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>9410.000000</td>\n",
       "      <td>4820.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>47.777600</td>\n",
       "      <td>-121.315000</td>\n",
       "      <td>6210.000000</td>\n",
       "      <td>871200.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              price      bedrooms     bathrooms   sqft_living      sqft_lot  \\\n",
       "count  2.161300e+04  21613.000000  21613.000000  21613.000000  2.161300e+04   \n",
       "mean   5.400881e+05      3.370842      2.114757   2079.899736  1.510697e+04   \n",
       "std    3.671272e+05      0.930062      0.770163    918.440897  4.142051e+04   \n",
       "min    7.500000e+04      0.000000      0.000000    290.000000  5.200000e+02   \n",
       "25%    3.219500e+05      3.000000      1.750000   1427.000000  5.040000e+03   \n",
       "50%    4.500000e+05      3.000000      2.250000   1910.000000  7.618000e+03   \n",
       "75%    6.450000e+05      4.000000      2.500000   2550.000000  1.068800e+04   \n",
       "max    7.700000e+06     33.000000      8.000000  13540.000000  1.651359e+06   \n",
       "\n",
       "             floors    waterfront          view     condition         grade  \\\n",
       "count  21613.000000  21613.000000  21613.000000  21613.000000  21613.000000   \n",
       "mean       1.494309      0.007542      0.234303      3.409430      7.656873   \n",
       "std        0.539989      0.086517      0.766318      0.650743      1.175459   \n",
       "min        1.000000      0.000000      0.000000      1.000000      1.000000   \n",
       "25%        1.000000      0.000000      0.000000      3.000000      7.000000   \n",
       "50%        1.500000      0.000000      0.000000      3.000000      7.000000   \n",
       "75%        2.000000      0.000000      0.000000      4.000000      8.000000   \n",
       "max        3.500000      1.000000      4.000000      5.000000     13.000000   \n",
       "\n",
       "         sqft_above  sqft_basement      yr_built  yr_renovated           lat  \\\n",
       "count  21613.000000   21613.000000  21613.000000  21613.000000  21613.000000   \n",
       "mean    1788.390691     291.509045   1971.005136     84.402258     47.560053   \n",
       "std      828.090978     442.575043     29.373411    401.679240      0.138564   \n",
       "min      290.000000       0.000000   1900.000000      0.000000     47.155900   \n",
       "25%     1190.000000       0.000000   1951.000000      0.000000     47.471000   \n",
       "50%     1560.000000       0.000000   1975.000000      0.000000     47.571800   \n",
       "75%     2210.000000     560.000000   1997.000000      0.000000     47.678000   \n",
       "max     9410.000000    4820.000000   2015.000000   2015.000000     47.777600   \n",
       "\n",
       "               long  sqft_living15     sqft_lot15  \n",
       "count  21613.000000   21613.000000   21613.000000  \n",
       "mean    -122.213896    1986.552492   12768.455652  \n",
       "std        0.140828     685.391304   27304.179631  \n",
       "min     -122.519000     399.000000     651.000000  \n",
       "25%     -122.328000    1490.000000    5100.000000  \n",
       "50%     -122.230000    1840.000000    7620.000000  \n",
       "75%     -122.125000    2360.000000   10083.000000  \n",
       "max     -121.315000    6210.000000  871200.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"Descripción\"\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ven las 18 columnas que contiene el dataframe, además podemos observar las características de cada una de ellas, como la cantidad de registros para cada una, el promedio, desviación estándar, mínimo, máximo y los percentiles 20, 50 y 75.\n",
    "\n",
    "Cada columna indica una característica de la casa en venta. Se puede observar que existen 21.613 registros de casas, en las cuales se cumple que no existen valores nulos y son valores númericos, donde la mayoría son enteros, excepto cinco características entre las que se encuentran los baños y pisos, lo que es extraño que no sean valores enteros. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Normalización de datos y transformación variable a predecir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: <> (c. Normalice los datos antes de trabajar y aplique una transformación adecuada a la variable a predecir. Explique la importancia/conveniencia de realizar estas dos operaciones.)\n",
    "\n",
    "A continuación se procede a normalizar los datos y aplicar una transformación a la variable precio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "df_scaled['price'] = np.log(df['price'])  #creo que esta es la transformación \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se creó un nuevo dataframe en base al anterior, pero normalizado. Ésta transformación centra los valores con media 0 y los escala en una varianza unitaria.\n",
    "\n",
    "Es importante mencionar que es conveniente hacer estas operaciones dado que en varias funciones objetivos del ámbito de maching learning, se utilizan elementos que asumen que todas las características están centradas en 0, como por ejemplo regularizadores y kernels. Además es importante que las varianzas estén en el mismo orden, pues de lo contrario alguna de las características podría predominar dentro de la función objetivo, y así la máquina no puede aprender correctamentemente las otras características.\n",
    "\n",
    "Por otro lado, se debe destacar la transformación posterior que se le realizó a la característica del precio, la cual consiste en una linealización de los datos al aplicar la función logarítmica. Esto es, ya que no se sabe a priori el comportamiento de la variable **precio**, dado que  puede ser cuadrática, cúbica o de otro grado, como la regresión precide una variable lineal, se tiene que dejar de esta forma para que el modelo propuesto sea válido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Regresión lineal de mínimos cuadrados básica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: <> (d Realice una regresión lineal de mínimos cuadrados básica. Explique la importancia/conveniencia del paso 4 y los argumentos que se deben entregar a la función que implementa la regresión lineal. http://sebastianraschka.com/Articles/2014_about_feature_scaling.html#standardization-and-min-max-scaling)\n",
    "\n",
    "Se procede a realizar una regresión líneal de mínimos cuadrados básica,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model as lm\n",
    "X = df_scaled.iloc[:,1:] #use .ix instead, in older pandas version\n",
    "N = X.shape[0]\n",
    "X.insert(X.shape[1], 'intercept', np.ones(N)) ##Agregamos interceptos a matriz X\n",
    "y = df_scaled['price']  #Variable transformada!!!!!\n",
    "#mascara estatica con el 70% de los datos\n",
    "mascara = np.zeros(len(X))\n",
    "limit = int(len(X)*0.7)\n",
    "mascara[:limit] = 1\n",
    "istrain = mascara== 1\n",
    "# Acá toma el 70% de los datos para entrenar.\n",
    "# Define el conjunto de entrenamiento\n",
    "Xtrain = X[istrain]\n",
    "ytrain = y[istrain]\n",
    "\n",
    "#Define el conjunto de test\n",
    "Xtest = X[np.logical_not(istrain)]\n",
    "ytest = y[np.logical_not(istrain)]\n",
    "\n",
    "linreg = lm.LinearRegression(fit_intercept = False)\n",
    "modelo = linreg.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante destacar, que en la línea 4 se añaden una nueva columna al dataframe con coeficientes unitarios la cual permitirá que se agreguen los interceptos sin alteración de estos en la regresión lineal.\n",
    "El argumento que se debe entregar a la función es fit_intercept con valor falso, dado que los interceptos ya fueron considerados según la explicación anterior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  e) Pesos y Z-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se presenta una tabla con los pesos y Z-score correspondientes a cada predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "def getStandardRegressionError(linreg, Xtrain , ytrain ):\n",
    "   std_reg = np.sqrt(((linreg.predict(Xtrain) - ytrain) ** 2).sum()/21594)\n",
    "   return std_reg\n",
    "\n",
    "def getZscore(linreg,std_reg,K):\n",
    "   inverse_covariance = np.linalg.inv(Xtrain.transpose().dot(Xtrain))\n",
    "   z_score = np.ones(K)\n",
    "   for i in range(0,K):\n",
    "      z_score[i] = linreg.coef_[i]/(std_reg *np.sqrt((inverse_covariance[i,i])))\n",
    "      \n",
    "   return z_score \n",
    "\n",
    "st_error = getStandardRegressionError(modelo,Xtrain,ytrain)\n",
    "#print(regresion_error)\n",
    "\n",
    "cantidad_variables = (len(modelo.coef_[:]))\n",
    "\n",
    "Z_score = getZscore(modelo,st_error,cantidad_variables)\n",
    "nombres = list(Xtrain.columns[:-1])\n",
    "nombres.append('intercept')\n",
    "def do_table(nombres,modelo,Z_score):\n",
    "    cantidad_variables = (len(modelo.coef_[:]))\n",
    "    for i in range(cantidad_variables):\n",
    "        print('<tr><td>'+nombres[i]+'</td><td>'+str(round(modelo.coef_[i],4))+'</td><td>'+str(round(Z_score[i],4))+'</td></tr>')\n",
    "    \n",
    "#do_table(nombres,modelo,Z_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "        <th>Atributo</th>\n",
    "        <th>Coeficiente</th>\n",
    "        <th>Z-score</th>\n",
    "</tr>\n",
    "<tr><td>bedrooms</td><td>-0.0071</td><td>-3.1244</td></tr>\n",
    "<tr><td>bathrooms</td><td>0.057</td><td>17.8574</td></tr>\n",
    "<tr><td>sqft_living</td><td>-3.54349922406e+12</td><td>-31427226.961</td></tr>\n",
    "<tr><td>sqft_lot</td><td>0.0225</td><td>9.2179</td></tr>\n",
    "<tr><td>floors</td><td>0.0372</td><td>14.5084</td></tr>\n",
    "<tr><td>waterfront</td><td>0.0333</td><td>17.5574</td></tr>\n",
    "<tr><td>view</td><td>0.0424</td><td>20.9556</td></tr>\n",
    "<tr><td>condition</td><td>0.0458</td><td>24.809</td></tr>\n",
    "<tr><td>grade</td><td>0.1844</td><td>56.904</td></tr>\n",
    "<tr><td>sqft_above</td><td>3.19491406164e+12</td><td>31427226.961</td></tr>\n",
    "<tr><td>sqft_basement</td><td>1.70752884084e+12</td><td>31427226.961</td></tr>\n",
    "<tr><td>yr_built</td><td>-0.1101</td><td>-40.4754</td></tr>\n",
    "<tr><td>yr_renovated</td><td>0.0146</td><td>8.1521</td></tr>\n",
    "<tr><td>lat</td><td>0.1858</td><td>102.2836</td></tr>\n",
    "<tr><td>long</td><td>-0.0043</td><td>-1.9904</td></tr>\n",
    "<tr><td>sqft_living15</td><td>0.0883</td><td>28.9337</td></tr>\n",
    "<tr><td>sqft_lot15</td><td>-0.0082</td><td>-3.2716</td></tr>\n",
    "<tr><td>intercept</td><td>13.0395</td><td>7435.1966</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condicionamiento de la matriz X: 3.20933985079e+15\n"
     ]
    }
   ],
   "source": [
    "print \"Condicionamiento de la matriz X:\", LA.cond(Xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando la tabla anterior, es posible dar cuenta de existen cosas anómalas en el resultado de la regresión lineal. Por ejemplo, los atributos *sqft_living, sqft_above y sqft_basement* tienen un peso y z-score demasiado alto en comparación al resto de las características. \n",
    "\n",
    "Lo anterior es atribuible al mal condicionamiento de la matriz, que es de $3,21 \\cdot 10^{15}$, valor que está muy lejana al 1 que es el valor que implica que la matriz esté bien condicionada. Este mal condicionamiento de la matriz puede ser porque existen características que son linealmente dependientes de otras. El gran valor del número de condicionamiento implica que esta matriz está muy cercana a hacer singular ( no invertible ) y el cálculo de su inversa, o solución de un sistema lineal de ecuaciones es propenso a grandes errores numéricos.\n",
    "\n",
    "Para ver las variables que están más correlacionadas con la respuesta, podemos suponer una hipótesis nula tal que  $H_{0} :B _{j}= 0$ , la cual se acepta o rechaza dependiendo de los valores de Z-scores:\n",
    "\n",
    "$z_{j}= \\frac{B_{j}}{\\lambda \\sqrt{v_{jj}}}$\n",
    "\n",
    "Donde,\n",
    "\n",
    "$v_{jj} = (X^{T}X)^{-1}_{jj}$\n",
    "\n",
    "Esto lo hacemos al 5% con la distribución t con 21593 grados de libertad, bajo esto, todo valor absoluto de Z-score mayor a 1.645 conllevará un rechazo a la hipotesis nula, implicando que su coeficiente es no nulo, por ende estará correlacionado con la respuesta. Con este criterio, todos los atributos serían relevantes pero está sesgado al mal condicionamiento de la matriz.\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) Método de corrección de lo observado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para corregir lo anterior, se podrían aplicar dos métodos por lo bajo:\n",
    "1. Encontrar las características linealmente dependientes de otras y eliminarlas.\n",
    "2. Aplicar un regularizador que afecte lo menos posible a la regresión lineal original, pero que aún así arregle los atributos inconsistentes.\n",
    "De las opciones anteriores se escogerá la segunda, debido que la primera requiere mucho costo computacional para encontrar las características linealmente dependientes, y además, el hecho de eliminar características por completo,  consideramos una medida demasiado radical.\n",
    "\n",
    "Por lo tanto, se aplica un regularizador de Ridge con un lambda lo más pequeño posible. Para obtener este pequeño valor, se obtienen los valores propios de la matriz original, así el más diminuto se considera como el parámetro de lambda para la regularización de Ridge.\n",
    "\n",
    "Se sabe que para obtener los valores propios de una matriz, ésta debe ser cuadrada, sin embargo muestra matriz es rectangular de 21613 x 18. Por lo tanto, se calculan los valores propios de la matriz original por su transpuesta. El resultado anterior, no dará el valor exacto del menos valor propio, pero si dará un valor aproximado de este, que es lo que buscamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "Z = Xtrain.transpose().dot(Xtrain)\n",
    "[V, D] = LA.eig(Z)\n",
    "min_eigenvalue =  min(V) # Equivale al valor propio de la matriz \n",
    "#https://stackoverflow.com/questions/22631956/how-to-find-eigenvalues-for-non-quadratic-matrix\n",
    "clf = Ridge(alpha=min_eigenvalue,fit_intercept=False)\n",
    "modelo_regularizado = clf.fit(Xtrain, ytrain)\n",
    "st_error_reg = getStandardRegressionError(modelo_regularizado,Xtrain,ytrain)\n",
    "Z_score_reg = getZscore(modelo_regularizado,st_error_reg,cantidad_variables)\n",
    "\n",
    "\n",
    "#for i in range(cantidad_variables):\n",
    "    #print(round(Z_score[i],2),round(modelo.coef_[i],2),\n",
    "          #round(Z_score_reg[i],2),round(modelo_regularizado.coef_[i],2))\n",
    "    \n",
    "#do_table(nombres,modelo_regularizado,Z_score_reg)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "        <th>Atributo</th>\n",
    "        <th>Coeficiente</th>\n",
    "        <th>Z-score</th>\n",
    "</tr>\n",
    "<tr><td>bedrooms</td><td>-0.0083</td><td>-3.6271</td></tr>\n",
    "<tr><td>bathrooms</td><td>0.056</td><td>17.5207</td></tr>\n",
    "<tr><td>sqft_living</td><td>0.4368</td><td>0.0</td></tr>\n",
    "<tr><td>sqft_lot</td><td>0.0222</td><td>9.0825</td></tr>\n",
    "<tr><td>floors</td><td>0.0376</td><td>14.6681</td></tr>\n",
    "<tr><td>waterfront</td><td>0.0335</td><td>17.6757</td></tr>\n",
    "<tr><td>view</td><td>0.0421</td><td>20.7899</td></tr>\n",
    "<tr><td>condition</td><td>0.0453</td><td>24.5509</td></tr>\n",
    "<tr><td>grade</td><td>0.1862</td><td>57.4635</td></tr>\n",
    "<tr><td>sqft_above</td><td>-0.2946</td><td>-0.0</td></tr>\n",
    "<tr><td>sqft_basement</td><td>-0.1509</td><td>-0.0</td></tr>\n",
    "<tr><td>yr_built</td><td>-0.1099</td><td>-40.3952</td></tr>\n",
    "<tr><td>yr_renovated</td><td>0.0148</td><td>8.2364</td></tr>\n",
    "<tr><td>lat</td><td>0.1863</td><td>102.5644</td></tr>\n",
    "<tr><td>long</td><td>-0.0041</td><td>-1.9006</td></tr>\n",
    "<tr><td>sqft_living15</td><td>0.0859</td><td>28.1753</td></tr>\n",
    "<tr><td>sqft_lot15</td><td>-0.007</td><td>-2.8055</td></tr>\n",
    "<tr><td>intercept</td><td>13.0397</td><td>7436.2456</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "### Se puede hacer una tabla con Z scores antes y despúes, dan z-scores muy bajos, cuales podrian ser los que están mas correlacionados?!\n",
    "Realizando la regresión lineal con el regularizador de Ridge, se puede ver mediante los nuevos pesos y z-score obtenidos que solo se modificaron los atributos que anteriormente tenían problemas, afectando de forma casi nula al resto, gracias al pequeño regularizador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g) Validación cruzada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo no regularizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse_test: 0.0654124075861\n",
      "mse_cv 0.0646902919559\n"
     ]
    }
   ],
   "source": [
    "#K=5\n",
    "yhat_test = linreg.predict(Xtest)\n",
    "mse_test = np.mean(np.power(yhat_test - ytest, 2))\n",
    "Xm = Xtrain.as_matrix()\n",
    "ym = ytrain.as_matrix()\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5)\n",
    "mse_cv = 0\n",
    "for train, val in kf.split(Xm):\n",
    "    linreg = lm.LinearRegression(fit_intercept = False)\n",
    "    linreg.fit(Xm[train], ym[train])\n",
    "    yhat_val = linreg.predict(Xm[val])\n",
    "    mse_fold = np.mean(np.power(yhat_val - ym[val], 2))\n",
    "    mse_cv += mse_fold\n",
    "mse_cv_5 = mse_cv / 5\n",
    "print \"mse_test:\",mse_test\n",
    "print \"mse_cv\",mse_cv_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse_test: 0.0653598742452\n",
      "mse_cv 0.0646905656615\n",
      "Diferencia k=5 y k=10 2.73705611634e-07\n"
     ]
    }
   ],
   "source": [
    "#K = 10\n",
    "\n",
    "yhat_test = linreg.predict(Xtest)\n",
    "mse_test = np.mean(np.power(yhat_test - ytest, 2))\n",
    "Xm = Xtrain.as_matrix()\n",
    "ym = ytrain.as_matrix()\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10)\n",
    "mse_cv = 0\n",
    "for train, val in kf.split(Xm):\n",
    "    linreg = lm.LinearRegression(fit_intercept = False)\n",
    "    linreg.fit(Xm[train], ym[train])\n",
    "    yhat_val = linreg.predict(Xm[val])\n",
    "    mse_fold = np.mean(np.power(yhat_val - ym[val], 2))\n",
    "    mse_cv += mse_fold\n",
    "mse_cv_10 = mse_cv / 10\n",
    "print \"mse_test:\",mse_test\n",
    "print \"mse_cv\",mse_cv_10\n",
    "print \"Diferencia k=5 y k=10\",(mse_cv_10 - mse_cv_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diferencia k=15 y k=10 1.02138864932e-05\n"
     ]
    }
   ],
   "source": [
    "yhat_test = linreg.predict(Xtest)\n",
    "mse_test = np.mean(np.power(yhat_test - ytest, 2))\n",
    "Xm = Xtrain.as_matrix()\n",
    "ym = ytrain.as_matrix()\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=15)\n",
    "mse_cv = 0\n",
    "for train, val in kf.split(Xm):\n",
    "    linreg = lm.LinearRegression(fit_intercept = False)\n",
    "    linreg.fit(Xm[train], ym[train])\n",
    "    yhat_val = linreg.predict(Xm[val])\n",
    "    mse_fold = np.mean(np.power(yhat_val - ym[val], 2))\n",
    "    mse_cv += mse_fold\n",
    "mse_cv_15 = mse_cv / 15\n",
    "print \"Diferencia k=15 y k=10\",(mse_cv_15 - mse_cv_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diferencia k=15 y k=10000 -2.00950435097e-06\n"
     ]
    }
   ],
   "source": [
    "yhat_test = linreg.predict(Xtest)\n",
    "mse_test = np.mean(np.power(yhat_test - ytest, 2))\n",
    "Xm = Xtrain.as_matrix()\n",
    "ym = ytrain.as_matrix()\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10000)\n",
    "mse_cv = 0\n",
    "for train, val in kf.split(Xm):\n",
    "    linreg = lm.LinearRegression(fit_intercept = False)\n",
    "    linreg.fit(Xm[train], ym[train])\n",
    "    yhat_val = linreg.predict(Xm[val])\n",
    "    mse_fold = np.mean(np.power(yhat_val - ym[val], 2))\n",
    "    mse_cv += mse_fold\n",
    "mse_cv_1000 = mse_cv / 10000\n",
    "print \"Diferencia k=15 y k=10000\",(mse_cv_1000 - mse_cv_15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo  regularizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse_test: 0.0653128519496\n",
      "mse_cv 0.0645164662505\n"
     ]
    }
   ],
   "source": [
    "#K=5\n",
    "yhat_test = linreg.predict(Xtest)\n",
    "mse_test = np.mean(np.power(yhat_test - ytest, 2))\n",
    "Xm = Xtrain.as_matrix()\n",
    "ym = ytrain.as_matrix()\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5)\n",
    "mse_cv = 0\n",
    "for train, val in kf.split(Xm):\n",
    "    clf = Ridge(alpha=min_eigenvalue,fit_intercept=False)\n",
    "    modelo_regularizado = clf.fit(Xtrain, ytrain)\n",
    "    yhat_val = modelo_regularizado.predict(Xm[val])\n",
    "    mse_fold = np.mean(np.power(yhat_val - ym[val], 2))\n",
    "    mse_cv += mse_fold\n",
    "mse_cv_5 = mse_cv / 5\n",
    "print \"mse_test:\",mse_test\n",
    "print \"mse_cv\",mse_cv_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse_test: 0.0653128519496\n",
      "mse_cv K=5: 0.0645164662505\n",
      "mse_cv K=10: 0.0645163259615\n",
      "Diferencia k=5 y k=10 -1.40288987552e-07\n",
      "diferencia k=5 y mse_test 0.000796385699151\n",
      "diferencia k=10 y mse_test 0.000796525988139\n"
     ]
    }
   ],
   "source": [
    "#K = 10\n",
    "yhat_test = linreg.predict(Xtest)\n",
    "mse_test = np.mean(np.power(yhat_test - ytest, 2))\n",
    "Xm = Xtrain.as_matrix()\n",
    "ym = ytrain.as_matrix()\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10)\n",
    "mse_cv = 0\n",
    "for train, val in kf.split(Xm):\n",
    "    clf = Ridge(alpha=min_eigenvalue,fit_intercept=False)\n",
    "    modelo_regularizado = clf.fit(Xtrain, ytrain)\n",
    "    yhat_val = modelo_regularizado.predict(Xm[val])\n",
    "    mse_fold = np.mean(np.power(yhat_val - ym[val], 2))\n",
    "    mse_cv += mse_fold\n",
    "mse_cv_10 = mse_cv / 10\n",
    "print \"mse_test:\",mse_test\n",
    "print \"mse_cv K=5:\",mse_cv_5\n",
    "print \"mse_cv K=10:\",mse_cv_10\n",
    "print \"Diferencia k=5 y k=10\",(mse_cv_10 - mse_cv_5)\n",
    "print \"diferencia k=5 y mse_test\",(mse_test - mse_cv_5)\n",
    "print \"diferencia k=10 y mse_test\",(mse_test - mse_cv_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En base a los errores de validación obtenidos, se puede concluir que al realizar validación cruzada con un 5 folds se obtiene un valor milésimas más cercano al error real del modelo que con 10 folds. Hay un cierto número de folds óptimo que puede predecir de mejor manera el error del modelo, luego de eso el error de validación se aleja del error real del modelo lentamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacer gráfico k=1 a 10 para encontrar k*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(h) Mida los errores de predicci´on para cada dato de entrenamiento. Utilizando un “quantile-quantile plot”\n",
    "determine si es razonable la hip´otesis de normalidad sobre los residuos del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEWCAYAAABBvWFzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmZmVhROKmZmVxf8HA6GtXJ9VCskAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xae19b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pylab \n",
    "import scipy.stats as stats\n",
    "yhat_test = linreg.predict(Xtest)\n",
    "\n",
    "stats.probplot(np.power(yhat_test - ytest, 2), dist=\"norm\", plot=pylab)\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncIiISEYUHCIikpH/B4gkaeqceK8OAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xfb12b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'normal'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-bc6232ae4de5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m    \u001b[1;32mprint\u001b[0m \u001b[1;34m\"r2:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mgetQQPlot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-35-bc6232ae4de5>\u001b[0m in \u001b[0;36mgetQQPlot\u001b[1;34m(Xtrain, ytrain)\u001b[0m\n\u001b[0;32m     11\u001b[0m    \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m    \u001b[0mprediction_error\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m    \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m    \u001b[0mnorm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m    \u001b[0mslope\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintercept\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinregress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction_error\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'module' object has no attribute 'normal'"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "def getQQPlot(Xtrain,ytrain):\n",
    "   predict = linreg.predict(Xtrain)\n",
    "   ytrain = np.asarray(ytrain)\n",
    "   prediction_error = predict - ytrain \n",
    "   \n",
    "   normal = stats.probplot(prediction_error, dist='norm', plot=plt)\n",
    "   plt.ylabel('Training Data Error')\n",
    "   plt.title('Quantile-Quantile Plot')\n",
    "   plt.show()\n",
    "   prediction_error.sort()\n",
    "   norm=random.normal(0,2,len(predict))\n",
    "   norm.sort()\n",
    "   slope, intercept, r_value, p_value, std_err = stats.linregress(prediction_error,norm)\n",
    "   print \"r2:\",r_value\n",
    "\n",
    "getQQPlot(Xtrain,ytrain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R^{2}: 0.9977$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Construya una funci\u0013on que implemente Forward Step-wise Selection (FSS). Es decir, partiendo con un\n",
    "modelo sin predictores (variables), agregue un predictor a la vez, re-ajustando el modelo de regresi\u0013on\n",
    "en cada paso. Para seleccionar localmente una variable, proponga/implemente un criterio distinto al\n",
    "utilizado en el c\u0013odigo de ejemplo. Construya un gr\u0013a\f",
    "co que muestre el error de entrenamiento y el error\n",
    "de pruebas como funci\u0013on del n\u0013umero de variables en el modelo. Ordene el eje x de menor a mayor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_regressors = X.columns[:-1]\n",
    "print names_regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error cuadrado\n",
    "\n",
    "def fss(x, y, names_x, k = 10000):\n",
    "    p = x.shape[1]-1\n",
    "    k = min(p, k)\n",
    "    names_x = np.array(names_x)\n",
    "    remaining = range(0, p)\n",
    "    selected = [p]\n",
    "    current_score = best_new_score = 0.0\n",
    "    while remaining and len(selected)<=k :\n",
    "        score_candidates = []\n",
    "        for candidate in remaining:\n",
    "            model = lm.LinearRegression(fit_intercept=False)\n",
    "            indexes = selected + [candidate]\n",
    "            x_train = x[:,indexes]\n",
    "            predictions_train = model.fit(x_train, y).predict(x_train)\n",
    "            residuals_train = predictions_train - y\n",
    "            mse_candidate = np.mean(np.power(residuals_train, 2))\n",
    "            score_candidates.append((mse_candidate, candidate))\n",
    "        score_candidates.sort()\n",
    "        score_candidates[:] = score_candidates[::-1]\n",
    "        best_new_score, best_candidate = score_candidates.pop()\n",
    "        remaining.remove(best_candidate)\n",
    "        selected.append(best_candidate)\n",
    "        print \"selected = %s ...\"%names_x[best_candidate]\n",
    "        print \"totalvars=%d, mse = %f\"%(len(indexes),best_new_score)\n",
    "    return selected\n",
    "names_regressors = X.columns[:-1] #without intercept\n",
    "fss(Xm,ym,names_regressors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error_Abs\n",
    "def fss_abs(x, y, names_x, k = 10000):\n",
    "    p = x.shape[1]-1\n",
    "    k = min(p, k)\n",
    "    names_x = np.array(names_x)\n",
    "    remaining = range(0, p)\n",
    "    selected = [p]\n",
    "    current_score = best_new_score = 0.0\n",
    "    while remaining and len(selected)<=k :\n",
    "        score_candidates = []\n",
    "        for candidate in remaining:\n",
    "            model = lm.LinearRegression(fit_intercept=False)\n",
    "            indexes = selected + [candidate]\n",
    "            x_train = x[:,indexes]\n",
    "            predictions_train = model.fit(x_train, y).predict(x_train)\n",
    "            residuals_train = predictions_train - y\n",
    "            mse_candidate = np.mean(np.fabs(residuals_train))\n",
    "            score_candidates.append((mse_candidate, candidate))\n",
    "        score_candidates.sort()\n",
    "        score_candidates[:] = score_candidates[::-1]\n",
    "        best_new_score, best_candidate = score_candidates.pop()\n",
    "        remaining.remove(best_candidate)\n",
    "        selected.append(best_candidate)\n",
    "        print \"selected = %s ...\"%names_x[best_candidate]\n",
    "        print \"totalvars=%d, mse = %f\"%(len(indexes),best_new_score)\n",
    "    return selected\n",
    "names_regressors = X.columns[:-1] #without intercept\n",
    "fss_abs(Xm,ym,names_regressors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fss_zscore(x, y, names_x, k = 10000):\n",
    "    p = x.shape[1]-1\n",
    "    k = min(p, k)\n",
    "    names_x = np.array(names_x)\n",
    "    remaining = range(0, p)\n",
    "    selected = [p]\n",
    "    current_score = best_new_score = 0.0\n",
    "    while remaining and len(selected)<=k :\n",
    "        score_candidates = []\n",
    "        for candidate in remaining:\n",
    "            model = lm.LinearRegression(fit_intercept=False)\n",
    "            indexes = selected + [candidate]\n",
    "            x_train = x[:,indexes]\n",
    "        \n",
    "            # Cambio\n",
    "            st_error = getStandardRegressionError(model,x_train,y)\n",
    "            cantidad_variables = (len(selected)+1)\n",
    "            zscore_candidate = abs(getZscore(modelo,st_error,cantidad_variables))[-1]\n",
    "            predictions_train = model.fit(x_train,y).predict(x_train)\n",
    "            residuals_train = predictions_train - y\n",
    "            zscore_train_residual = np.mean(np.power(residuals_train,2))            \n",
    "            score_candidates.append((zscore_candidate,candidate))\n",
    "            \n",
    "        score_candidates.sort()\n",
    "        score_candidates[:] = score_candidates[::-1]\n",
    "        best_new_score, best_candidate = score_candidates.pop()\n",
    "        remaining.remove(best_candidate)\n",
    "        selected.append(best_candidate)\n",
    "        print \"selected = %s ...\"%names_x[best_candidate]\n",
    "        print \"totalvars=%d, mse = %f\"%(len(indexes),best_new_score)\n",
    "    return selected\n",
    "names_regressors = X.columns[:-1] #without intercept\n",
    "fss_zscore(Xm,ym,names_regressors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fss_mse( x , y , xtest, ytest, names_x, k = 10000):\n",
    "   p = x.shape[1] - 1\n",
    "   k = min(p,k)\n",
    "   names_x = np.array(names_x)\n",
    "   remaining = range(1,p+1)\n",
    "   training_error = []\n",
    "   test_error = []\n",
    "   selected = [0]\n",
    "   current_score = 0.0\n",
    "   best_new_score = 0.0\n",
    "\n",
    "   while remaining and len(selected) <= k :\n",
    "      score_candidates = []\n",
    "      for candidate in remaining:\n",
    "         model = lm.LinearRegression(fit_intercept = False)\n",
    "         indexes = selected + [candidate]\n",
    "\n",
    "         x_train = x[:,indexes]\n",
    "         x_test = xtest[:,indexes]\n",
    "\n",
    "         predictions_train = model.fit(x_train,y).predict(x_train)\n",
    "         residuals_train = predictions_train - y\n",
    "\n",
    "         predictions_test = model.fit(x_train,y).predict(x_test)\n",
    "         residuals_test= predictions_test - ytest\n",
    "         mse_train_candidate = np.mean(np.power(residuals_train,2))\n",
    "         mse_test_candidate = np.mean(np.power(residuals_test,2))\n",
    "         score_candidates.append((mse_train_candidate, mse_test_candidate ,candidate))\n",
    "\n",
    "      score_candidates.sort()\n",
    "      score_candidates[:] = score_candidates[::-1]\n",
    "      best_new_score , test_score, best_candidate = score_candidates.pop()\n",
    "      training_error.append(best_new_score)\n",
    "      test_error.append(test_score)\n",
    "      remaining.remove(best_candidate)\n",
    "      selected.append(best_candidate)\n",
    "      print  \"selected = %s ...\" %names_x[best_candidate-1]\n",
    "      print  \"totalvars = %d, mae = %f \" %(len(indexes), best_new_score)\n",
    "   return selected , training_error, test_error\n",
    "\n",
    "#\n",
    "#Forward Step-Wise Selection with Mean Absolute Error\n",
    "#\n",
    "def fss_mae( x , y , xtest, ytest, names_x, k = 10000):\n",
    "   p = x.shape[1] - 1\n",
    "   k = min(p,k)\n",
    "   names_x = np.array(names_x)\n",
    "   remaining = range(1,p+1)\n",
    "   training_error = []\n",
    "   test_error = []\n",
    "   selected = [0]\n",
    "   current_score = 0.0\n",
    "   best_new_score = 0.0\n",
    "\n",
    "   while remaining and len(selected) <= k :\n",
    "      score_candidates = []\n",
    "      for candidate in remaining:\n",
    "         model = lm.LinearRegression(fit_intercept = False) \n",
    "         indexes = selected + [candidate]\n",
    "         x_train = x[:,indexes]\n",
    "         x_test = xtest[:,indexes]\n",
    "\n",
    "         predictions_train = model.fit(x_train,y).predict(x_train)\n",
    "         residuals_train = np.fabs(predictions_train - y)\n",
    "\n",
    "         predictions_test = model.fit(x_train,y).predict(x_test)\n",
    "         residuals_test = np.fabs(predictions_test - ytest)\n",
    "\n",
    "         mae_training_candidate = np.mean(residuals_train)\n",
    "         mae_test_candidate = np.mean(residuals_test)\n",
    "         score_candidates.append((mae_training_candidate,mae_test_candidate , candidate))\n",
    "\n",
    "      score_candidates.sort()\n",
    "      score_candidates[:] = score_candidates[::-1]\n",
    "      best_new_score , test_score ,best_candidate = score_candidates.pop()\n",
    "      training_error.append(best_new_score)\n",
    "      test_error.append(test_score)\n",
    "      remaining.remove(best_candidate)\n",
    "      selected.append(best_candidate)\n",
    "      print  \"selected = %s ...\" %names_x[best_candidate-1]\n",
    "      print  \"totalvars = %d, mse = %f \" %(len(indexes), best_new_score)\n",
    "   return selected , training_error, test_error\n",
    "#\n",
    "#Forward Step-Wise Selection with Z-score\n",
    "#\n",
    "def fss_zscore( x , y , xtest, ytest, names_x, k = 10000):\n",
    "   p = x.shape[1] - 1\n",
    "   k = min(p,k)\n",
    "   names_x = np.array(names_x)\n",
    "   remaining = range(1,p+1)\n",
    "   selected = [0]\n",
    "   training_error = []\n",
    "   test_error = []\n",
    "   current_score = 0.0\n",
    "   best_new_score = 0.0\n",
    "\n",
    "   while remaining and len(selected) <= k :\n",
    "      score_candidates = []\n",
    "      for candidate in remaining:\n",
    "         model = lm.LinearRegression(fit_intercept = False)\n",
    "         indexes = selected + [candidate]\n",
    "         \n",
    "         x_train = x[:,indexes]\n",
    "         x_test = xtest[:,indexes]\n",
    "         \n",
    "         linreg = model.fit(x_train,y)\n",
    "         std_reg = getStandardRegressionError(linreg, x_train , y )\n",
    "         \n",
    "         zscore_candidate = abs(getZscore(linreg,std_reg,len(selected)+1)[-1])\n",
    "\n",
    "         predictions_train = model.fit(x_train,y).predict(x_train)\n",
    "         residuals_train = predictions_train - y\n",
    "\n",
    "         predictions_test = model.fit(x_train,y).predict(x_test)\n",
    "         residuals_test = predictions_test - ytest\n",
    "\n",
    "         zscore_train_residual = np.mean(np.power(residuals_train,2))\n",
    "         zscore_test_residual = np.mean(np.power(residuals_test,2))\n",
    "   \n",
    "         score_candidates.append((zscore_candidate,zscore_train_residual,zscore_test_residual ,candidate))\n",
    "\n",
    "      score_candidates.sort()\n",
    "      best_new_score , train_residual, test_residual, best_candidate = score_candidates.pop()\n",
    "      training_error.append(train_residual)\n",
    "      test_error.append(test_residual)\n",
    "      remaining.remove(best_candidate)\n",
    "\n",
    "      selected.append(best_candidate)\n",
    "      print  \"selected = %s ...\" %names_x[best_candidate-1]\n",
    "      print  \"totalvars = %d, zscore = %f \" %(len(indexes), best_new_score)\n",
    "   return selected , training_error, test_error\n",
    "\n",
    "def plotFSS(train_error,test_error, method):\n",
    "    p = range(1,18)\n",
    "    plt.plot(p,train_error, color=\"green\", linewidth=3, label = \"Training Set\")\n",
    "    plt.plot(p,test_error, color=\"red\",linewidth=3, label = \"Testing set\")\n",
    "    plt.legend()\n",
    "    axes = plt.gca()\n",
    "    if method == \"mse\":\n",
    "        plt.xlabel('Numero de variables')\n",
    "        plt.ylabel('Error MSE')\n",
    "        plt.title('FFS MSE ')\n",
    "        #axes.set_ylim([0.3,0.8])\n",
    "    \n",
    "    if method == \"mae\":\n",
    "        plt.xlabel('Numero de variables')\n",
    "        plt.ylabel('FFS MAE')\n",
    "        plt.title('FFS MAE')\n",
    "        #axes.set_ylim([0.4,0.7])\n",
    "        \n",
    "    if method == \"zscore\":\n",
    "        plt.xlabel('Numero de variables')\n",
    "        plt.ylabel('FFS Z-score')\n",
    "        plt.title('FFS Zscore')\n",
    "        axes.set_xlim([0,8])\n",
    "    plt.show()\n",
    "\n",
    "selected_mse , train_error_mse , test_error_mse  = fss_mse( Xtrain.as_matrix() ,ytrain.as_matrix() , Xtest.as_matrix() ,ytest.as_matrix() ,names_regressors )\n",
    "\n",
    "selected_mae , train_error_mae, test_error_mae = fss_mae( Xtrain.as_matrix() ,ytrain.as_matrix() , Xtest.as_matrix() ,ytest.as_matrix() ,names_regressors )\n",
    "\n",
    "selected_z , train_error_z, test_error_z = fss_zscore( Xtrain.as_matrix() ,ytrain.as_matrix() , Xtest.as_matrix() ,ytest.as_matrix() ,names_regressors )\n",
    "\n",
    "plotFSS(train_error_mse , test_error_mse, \"mse\")\n",
    "plotFSS(train_error_mae , test_error_mae, \"mae\")\n",
    "plotFSS(train_error_z , test_error_z, \"zscore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression reduce los coeficientes de regresión con una variable $\\lambda$ ,imponiendo un penalty al valor de cada coeficiente, mientras mas grande sea $\\lambda$ , mayor será la reducción, por lo que hay buscar un valor de $\\lambda$ eficiente, dado que si $\\lambda$ es muy grande, el modelo puede sufrir <strong>underfitting</strong> o un bias elevado, mientras que un $\\lambda$ demasiado bajo, puede no afectar al modelo en absoluto y no observar cambios en el <strong>overffiting</strong>.\n",
    "<br>\n",
    "<br>\n",
    "Para aplicar Ridge Regressión es necesario que los datos estén estandarizados y eliminar el intercepto en la penalización, dado que si se penaliza el intercepto, se generará un procedimiento dependiente del origen del eje $Y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "import matplotlib.pylab as plt\n",
    "X2 = X.drop('intercept', axis=1,inplace=False)\n",
    "Xtrain = X2[istrain]\n",
    "ytrain = y[istrain]\n",
    "names_regressors = X2.columns\n",
    "alphas_ = np.logspace(7,1,base=10)\n",
    "coefs = []\n",
    "model = Ridge(fit_intercept=True,solver='svd')\n",
    "for a in alphas_:\n",
    "    model.set_params(alpha=a)\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    coefs.append(model.coef_)\n",
    "    ax = plt.gca()\n",
    "for y_arr, label in zip(np.squeeze(coefs).T, names_regressors):\n",
    "    plt.plot(alphas_, y_arr, label=label)\n",
    "plt.legend()\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(ax.get_xlim()[::-1]) # reverse axis\n",
    "plt.title('Regularization Path RIDGE')\n",
    "plt.axis('tight')\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "import matplotlib.pylab as plt\n",
    "X2 = X.drop('intercept', axis=1,inplace=False)\n",
    "Xtrain = X2[istrain]\n",
    "ytrain = y[istrain]\n",
    "names_regressors = X2.columns\n",
    "alphas_ = np.logspace(0,-3,base=10)\n",
    "coefs = []\n",
    "model = Lasso(fit_intercept=True)\n",
    "for a in alphas_:\n",
    "    model.set_params(alpha=a)\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    coefs.append(model.coef_)\n",
    "    ax = plt.gca()\n",
    "for y_arr, label in zip(np.squeeze(coefs).T, names_regressors):\n",
    "    plt.plot(alphas_, y_arr)\n",
    "plt.legend()\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(ax.get_xlim()[::-1]) # reverse axis\n",
    "plt.title('Regularization Path LASSO')\n",
    "plt.axis('tight')\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = X2[np.logical_not(istrain)]\n",
    "ytest = y[np.logical_not(istrain)]\n",
    "alphas_ = np.logspace(7,1,base=10)\n",
    "coefs = []\n",
    "model = Ridge(fit_intercept=True,solver='svd')\n",
    "mse_test = []\n",
    "mse_train = []\n",
    "for a in alphas_:\n",
    "    model.set_params(alpha=a)\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    yhat_train = model.predict(Xtrain)\n",
    "    yhat_test = model.predict(Xtest)\n",
    "    mse_train.append(np.mean(np.power(yhat_train - ytrain, 2)))\n",
    "    mse_test.append(np.mean(np.power(yhat_test - ytest, 2)))\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas_,mse_train,label='train error ridge')\n",
    "ax.plot(alphas_,mse_test,label='test error ridge')\n",
    "plt.legend(loc=1)\n",
    "plt.title('Regularization Path RIDGE')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(ax.get_xlim()[::-1])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# LASSO\n",
    "Xtest = X2[np.logical_not(istrain)]\n",
    "ytest = y[np.logical_not(istrain)]\n",
    "alphas_ = np.logspace(0,-3,base=10)\n",
    "coefs = []\n",
    "model = Lasso(fit_intercept=True)\n",
    "mse_test = []\n",
    "mse_train = []\n",
    "for a in alphas_:\n",
    "    model.set_params(alpha=a)\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    yhat_train = model.predict(Xtrain)\n",
    "    yhat_test = model.predict(Xtest)\n",
    "    mse_train.append(np.mean(np.power(yhat_train - ytrain, 2)))\n",
    "    mse_test.append(np.mean(np.power(yhat_test - ytest, 2)))\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas_,mse_train,label='train error lasso')\n",
    "ax.plot(alphas_,mse_test,label='test error lasso')\n",
    "plt.legend(loc=1)\n",
    "plt.title('Regularization Path LASSO')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(ax.get_xlim()[::-1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Estimación parámetro de regularización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y,yhat): return np.mean(np.power(y-yhat,2))\n",
    "Xm = Xtrain.as_matrix()\n",
    "ym = ytrain.as_matrix()\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10)\n",
    "best_cv_mse = float(\"inf\")\n",
    "model = Ridge(fit_intercept=True,solver='svd')\n",
    "alphas_ = np.logspace(7,1,base=10)\n",
    "#print(largo)\n",
    "#print(alphas_)\n",
    "for a in alphas_:\n",
    "    model.set_params(alpha=a)\n",
    "    mse_list_k10 = [MSE(model.fit(Xm[train], ym[train]).predict(Xm[val]), ym[val]) \\\n",
    "        for train, val in kf.split(Xm)]\n",
    "    if np.mean(mse_list_k10) < best_cv_mse:\n",
    "        best_cv_mse = np.mean(mse_list_k10)\n",
    "        best_alpha = a\n",
    "\n",
    "print \"BEST PARAMETER=%f, MSE(CV)=%f\"%(best_alpha,best_cv_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_A = df_scaled.sample(1000,random_state=11)\n",
    "frames = []\n",
    "valor = df_scaled.price\n",
    "length = 0.3\n",
    "for z in np.arange(int(np.min(valor)),int(np.max(valor))+1,length):\n",
    "    #un maximo de 100 datos por intervalo\n",
    "    aux = df_scaled[(df_scaled.price >= z) & (df_scaled.price < z+length)].head(100)\n",
    "    frames.append(aux)\n",
    "df_B = pd.concat(frames).sample(1000,random_state=11) #crea el dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta secci\u0013on se presentar\u0013an dos muestras del dataframe utilizado en la actividades anteriores, donde cada\n",
    "una de estas tiene una propiedad distinta ya que son muestreadas en funci\u0013on del valor a predecir (logaritmo\n",
    "del precio de la casa). Por una parte se tiene una peque~na muestra A, la cual es extra\u0013\u0010da directamente de\n",
    "los datos con los que se trabaja (manteniendo la distribuci\u0013on de esta) y la muestra B, es generada con el\n",
    "prop\u0013osito de que en cada intervalo del rango de valores haya la misma cantidad de datos aproximadamente\n",
    "(simulando una distribuci\u0013on uniforme). El objetivo es familiarizarse con el concepto de Transfer Learning[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_A = df_A.iloc[:,1:].values\n",
    "y_A = df_A.price\n",
    "X_B = df_B.iloc[:,1:].values\n",
    "y_B = df_B.price\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain_A,Xval_A,ytrain_A,yval_A = train_test_split(X_A, y_A, test_size=0.3, random_state=42)\n",
    "Xtrain_B,Xval_B,ytrain_B,yval_B = train_test_split(X_B, y_B, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval\u0013ue los dos modelo de regresi\u0013on lineal que se generan al entrenar con cada muestra. Mida el error\n",
    "de cada modelo sobre ambos conjuntos de validaci\u0013on (A y B). Explique lo que observa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = lm.LinearRegression(fit_intercept = True)\n",
    "\n",
    "modelo_A = linreg.fit(Xtrain_A, ytrain_A)\n",
    "yhat_val = linreg.predict(Xval_A)\n",
    "val_errorA = np.mean(np.power(yhat_val - yval_A, 2))\n",
    "print \"Error de validación conjunto A:\",val_errorA\n",
    "\n",
    "modelo_B = linreg.fit(Xtrain_B, ytrain_B)\n",
    "yhat_val = linreg.predict(Xval_B)\n",
    "val_errorB = np.mean(np.power(yhat_val - yval_B, 2))\n",
    "print \"Error de validación conjunto B:\",val_errorB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eligiriamos el modelo A, dado que que experimentalmente el error de validación del conjunto A es menor que el del conjunto B, lo que implica que a con nueva data, la predicción del modelo A será más cercana que la del modelo B. Distribución normal de datos..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "En el \u0013area de la salud, diagnosticar a una persona de una enfermedad de forma r\u0013apida y correcta puede llegar\n",
    "a salvarle la vida. Los encargados de realizar estos diagn\u0013osticos, son m\u0013edicos que, observando ex\u0013amenes y\n",
    "ciertos indicadores, pueden concluir qu\u0013e enfermedad presenta el paciente. Si \u0013el medico se llegase a equivocar,\n",
    "aparte de que el paciente pueda perder la vida, el medico podr\u0013\u0010a ser demandado por negligencia arriesgando\n",
    "a~nos de c\u0013arcel o pagar sumas de dinero considerable, es por estas razones que es importante no cometer\n",
    "errores.\n",
    "Pong\u0013amonos en el contexto de que usted es contratado para generar un modelo que prediga si es que un\n",
    "paciente presenta una enfermedad cardiaca a partir de ciertos indicadores, tales como la edad, sexo, presi\u0013on"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
